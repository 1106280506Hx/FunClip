#!/usr/bin/env python3
# -*- encoding: utf-8 -*-
# Copyright FunASR (https://github.com/alibaba-damo-academy/FunClip). All Rights Reserved.
#  MIT License  (https://opensource.org/licenses/MIT)

from http import server
import os
import logging
import argparse
import gradio as gr
from funasr import AutoModel
from videoclipper import VideoClipper
from llm.openai_api import openai_call
from llm.qwen_api import call_qwen_model
from llm.g4f_openai_api import g4f_openai_call
from utils.trans_utils import extract_timestamps
from introduction import top_md_1, top_md_3, top_md_4
from utils.preview_components import create_integrated_preview_export_ui
import time
import logging
from accelerate.logging import get_logger
import sys

logger = get_logger(__name__)
if not logger.logger.handlers:
    logger.logger.setLevel(logging.INFO)
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.logger.addHandler(handler)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='argparse testing')
    parser.add_argument('--lang', '-l', type=str, default = "zh", help="language")
    parser.add_argument('--share', '-s', action='store_true', help="if to establish gradio share link")
    parser.add_argument('--port', '-p', type=int, default=7860, help='port number')
    parser.add_argument('--listen', action='store_true', help="if to listen to all hosts")
    args = parser.parse_args()
    
    if args.lang == 'zh':
        funasr_model = AutoModel(model="iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
                                vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                                punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
                                spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
                                )
    else:
        funasr_model = AutoModel(model="iic/speech_paraformer_asr-en-16k-vocab4199-pytorch",
                                vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                                punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
                                spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
                                )
    audio_clipper = VideoClipper(funasr_model)
    audio_clipper.lang = args.lang
    
    # Initialize Video Semantic Understander
    audio_clipper.init_semantic_understander("/remote-home/share/huggingface/Qwen3-VL-8B-Instruct")

    server_name='127.0.0.1'
    if args.listen:
        server_name = '0.0.0.0'
        
    # 1. å®šä¹‰åŒ…è£…å‡½æ•°ï¼Œå¤„ç† semantic_understand è¿”å›çš„ä¸¤ä¸ªå€¼
    def video_semantic_understanding_wrapper(video_input):
        if video_input is None:
            return "Please upload a video first.", None
        # è°ƒç”¨ videoclipperï¼Œè·å– (UIæ–‡æœ¬, ç»“æ„åŒ–æ•°æ®)
        ui_text, raw_data = audio_clipper.semantic_understand(video_input)
        return ui_text, raw_data

    # 2. å®šä¹‰åŒ…è£…å‡½æ•°ï¼Œå°† State ä¸­çš„æ•°æ®ä¼ ç»™ generate_musical_video
    # [ä¿®æ”¹] ç§»é™¤äº† tsv å‚æ•°
    def run_music(video, m_root, semantic_state, custom_audio):
        if not video: 
            return None, "No video input"
        
        if semantic_state is None:
            return None, "âš ï¸ è¯·å…ˆç‚¹å‡» 'æå–è§†é¢‘è¯­ä¹‰' æŒ‰é’®è·å–åˆ†æç»“æœï¼Œå†ç”Ÿæˆé…ä¹ã€‚"
            
        timestamp = int(time.time())
        base_name = os.path.splitext(video)[0]
        out_path = f"{base_name}_musical_{timestamp}.mp4"
        
        print(f"ğŸ”„ Re-generating music video... Output: {out_path}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸Šä¼ äº†è‡ªå®šä¹‰éŸ³é¢‘
        custom_path = None
        if custom_audio is not None:
            custom_path = custom_audio # Gradio è¿”å›çš„æ˜¯æ–‡ä»¶è·¯å¾„
            print(f"ğŸµ Using custom audio: {custom_path}")
        
        path, msg = audio_clipper.generate_musical_video(
            video_path=video, 
            music_root=m_root, 
            output_path=out_path, 
            shots_data_wrapper=semantic_state,
            custom_bgm_path=custom_path # [æ–°å¢å‚æ•°]
        )
        return path, f"âœ… ç”ŸæˆæˆåŠŸ (ç‰ˆæœ¬ {timestamp})\n{msg}"

    def audio_recog(audio_input, sd_switch, hotwords, output_dir):
        return audio_clipper.recog(audio_input, sd_switch, None, hotwords, output_dir=output_dir)

    def video_recog(video_input, sd_switch, hotwords, output_dir):
        return audio_clipper.video_recog(video_input, sd_switch, hotwords, output_dir=output_dir)

    def video_clip(dest_text, video_spk_input, start_ost, end_ost, state, output_dir):
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, dest_spk=video_spk_input, output_dir=output_dir
            )

    def mix_recog(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(
                video_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(
                audio_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state
    
    def mix_recog_speaker(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(
                video_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(
                audio_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state
    
    def mix_clip(dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, dest_spk=video_spk_input, output_dir=output_dir)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, dest_spk=video_spk_input, output_dir=output_dir)
            return None, (sr, res_audio), message, clip_srt
    
    def video_clip_addsub(dest_text, video_spk_input, start_ost, end_ost, state, output_dir, font_size, font_color):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, 
            font_size=font_size, font_color=font_color, 
            add_sub=True, dest_spk=video_spk_input, output_dir=output_dir
            )
        
    def llm_inference(system_content, user_content, srt_text, model, apikey):
        SUPPORT_LLM_PREFIX = ['qwen', 'gpt', 'g4f', 'moonshot', 'deepseek']
        if model.startswith('qwen'):
            return call_qwen_model(apikey, model, user_content+'\n'+srt_text, system_content)
        if model.startswith('gpt') or model.startswith('moonshot') or model.startswith('deepseek'):
            return openai_call(apikey, model, system_content, user_content+'\n'+srt_text)
        elif model.startswith('g4f'):
            model = "-".join(model.split('-')[1:])
            return g4f_openai_call(model, system_content, user_content+'\n'+srt_text)
        else:
            logging.error("LLM name error, only {} are supported as LLM name prefix."
                          .format(SUPPORT_LLM_PREFIX))
    
    def AI_clip(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return None, (sr, res_audio), message, clip_srt
    
    def AI_clip_subti(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return None, (sr, res_audio), message, clip_srt
    
    def video_semantic_understanding(video_input):
        if video_input is None:
            return "Please upload a video first."
        return audio_clipper.semantic_understand(video_input)

    # gradio interface
    theme = gr.Theme.load("funclip/utils/theme.json")
    with gr.Blocks(theme=theme) as funclip_service:
        gr.Markdown(top_md_1)
        # gr.Markdown(top_md_2)
        gr.Markdown(top_md_3)
        gr.Markdown(top_md_4)
        semantic_state_data = gr.State()
        video_state, audio_state = gr.State(), gr.State()
        with gr.Row():
            with gr.Column():
                with gr.Row():
                    video_input = gr.Video(label="è§†é¢‘è¾“å…¥ | Video Input")
                    audio_input = gr.Audio(label="éŸ³é¢‘è¾“å…¥ | Audio Input")
                with gr.Column():
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A4%9A%E8%AF%BB%E4%B9%A6%EF%BC%9F%E8%BF%99%E6%98%AF%E6%88%91%E5%90%AC%E8%BF%87%E6%9C%80%E5%A5%BD%E7%9A%84%E7%AD%94%E6%A1%88-%E7%89%87%E6%AE%B5.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/2022%E4%BA%91%E6%A0%96%E5%A4%A7%E4%BC%9A_%E7%89%87%E6%AE%B52.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%BD%BF%E7%94%A8chatgpt_%E7%89%87%E6%AE%B5.mp4'],
                                [video_input],
                                label='ç¤ºä¾‹è§†é¢‘ | Demo Video')
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E8%AE%BF%E8%B0%88.mp4'],
                                [video_input],
                                label='å¤šè¯´è¯äººç¤ºä¾‹è§†é¢‘ | Multi-speaker Demo Video')
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E9%B2%81%E8%82%83%E9%87%87%E8%AE%BF%E7%89%87%E6%AE%B51.wav'],
                                [audio_input],
                                label="ç¤ºä¾‹éŸ³é¢‘ | Demo Audio")
                    with gr.Column():
                        # with gr.Row():
                            # video_sd_switch = gr.Radio(["No", "Yes"], label="ğŸ‘¥åŒºåˆ†è¯´è¯äºº Get Speakers", value='No')
                        hotwords_input = gr.Textbox(label="ğŸš’ çƒ­è¯ | Hotwords(å¯ä»¥ä¸ºç©ºï¼Œå¤šä¸ªçƒ­è¯ä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼Œä»…æ”¯æŒä¸­æ–‡çƒ­è¯)")
                        output_dir = gr.Textbox(label="ğŸ“ æ–‡ä»¶è¾“å‡ºè·¯å¾„ | File Output Dir (å¯ä»¥ä¸ºç©ºï¼ŒLinux, macç³»ç»Ÿå¯ä»¥ç¨³å®šä½¿ç”¨)", value=" ")
                        with gr.Row():
                            recog_button = gr.Button("ğŸ‘‚ è¯†åˆ« | ASR", variant="primary")
                            recog_button2 = gr.Button("ğŸ‘‚ğŸ‘« è¯†åˆ«+åŒºåˆ†è¯´è¯äºº | ASR+SD")
                video_text_output = gr.Textbox(label="âœï¸ è¯†åˆ«ç»“æœ | Recognition Result")
                video_srt_output = gr.Textbox(label="ğŸ“– SRTå­—å¹•å†…å®¹ | RST Subtitles")
                video_semantic_output = gr.Textbox(label="ğŸ‘ï¸ è§†é¢‘è¯­ä¹‰ç†è§£ | Video Semantic Understanding")
                semantic_button = gr.Button("ğŸ‘ï¸ æå–è§†é¢‘è¯­ä¹‰ | Extract Semantic Tags")
            with gr.Column():
                with gr.Tab("ğŸ§  LLMæ™ºèƒ½è£å‰ª | LLM Clipping"):
                    with gr.Column():
                        prompt_head = gr.Textbox(label="Prompt System (æŒ‰éœ€æ›´æ”¹ï¼Œæœ€å¥½ä¸è¦å˜åŠ¨ä¸»ä½“å’Œè¦æ±‚)", value=("ä½ æ˜¯ä¸€ä¸ªè§†é¢‘srtå­—å¹•åˆ†æå‰ªè¾‘å™¨ï¼Œè¾“å…¥è§†é¢‘çš„srtå­—å¹•ï¼Œ"
                                "åˆ†æå…¶ä¸­çš„ç²¾å½©ä¸”å°½å¯èƒ½è¿ç»­çš„ç‰‡æ®µå¹¶è£å‰ªå‡ºæ¥ï¼Œè¾“å‡ºå››æ¡ä»¥å†…çš„ç‰‡æ®µï¼Œå°†ç‰‡æ®µä¸­åœ¨æ—¶é—´ä¸Šè¿ç»­çš„å¤šä¸ªå¥å­åŠå®ƒä»¬çš„æ—¶é—´æˆ³åˆå¹¶ä¸ºä¸€æ¡ï¼Œ"
                                "æ³¨æ„ç¡®ä¿æ–‡å­—ä¸æ—¶é—´æˆ³çš„æ­£ç¡®åŒ¹é…ã€‚è¾“å‡ºéœ€ä¸¥æ ¼æŒ‰ç…§å¦‚ä¸‹æ ¼å¼ï¼š1. [å¼€å§‹æ—¶é—´-ç»“æŸæ—¶é—´] æ–‡æœ¬ï¼Œæ³¨æ„å…¶ä¸­çš„è¿æ¥ç¬¦æ˜¯â€œ-â€"))
                        prompt_head2 = gr.Textbox(label="Prompt Userï¼ˆä¸éœ€è¦ä¿®æ”¹ï¼Œä¼šè‡ªåŠ¨æ‹¼æ¥å·¦ä¸‹è§’çš„srtå­—å¹•ï¼‰", value=("è¿™æ˜¯å¾…è£å‰ªçš„è§†é¢‘srtå­—å¹•ï¼š"))
                        with gr.Column():
                            with gr.Row():
                                llm_model = gr.Dropdown(
                                    choices=["gpt-5",
                                        "deepseek-chat"
                                        "qwen-plus",
                                             "gpt-3.5-turbo", 
                                             "gpt-3.5-turbo-0125", 
                                             "gpt-4-turbo",
                                             "g4f-gpt-3.5-turbo"], 
                                    value="deepseek-chat",
                                    label="LLM Model Name",
                                    allow_custom_value=True)
                                apikey_input = gr.Textbox(label="APIKEY")
                            llm_button =  gr.Button("LLMæ¨ç† | LLM Inferenceï¼ˆé¦–å…ˆè¿›è¡Œè¯†åˆ«ï¼Œég4féœ€é…ç½®å¯¹åº”apikeyï¼‰", variant="primary")
                        llm_result = gr.Textbox(label="LLM Clipper Result")
                        with gr.Row():
                            llm_clip_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª | AI Clip", variant="primary")
                            llm_clip_subti_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª+å­—å¹• | AI Clip+Subtitles")
                with gr.Tab("âœ‚ï¸ æ ¹æ®æ–‡æœ¬/è¯´è¯äººè£å‰ª | Text/Speaker Clipping"):
                    video_text_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªæ–‡æœ¬ | Text to Clip (å¤šæ®µæ–‡æœ¬ä½¿ç”¨'#'è¿æ¥)")
                    video_spk_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªè¯´è¯äºº | Speaker to Clip (å¤šä¸ªè¯´è¯äººä½¿ç”¨'#'è¿æ¥)")
                    with gr.Row():
                        clip_button = gr.Button("âœ‚ï¸ è£å‰ª | Clip", variant="primary")
                        clip_subti_button = gr.Button("âœ‚ï¸ è£å‰ª+å­—å¹• | Clip+Subtitles")
                    with gr.Row():
                        video_start_ost = gr.Slider(minimum=-500, maximum=1000, value=0, step=50, label="âª å¼€å§‹ä½ç½®åç§» | Start Offset (ms)")
                        video_end_ost = gr.Slider(minimum=-500, maximum=1000, value=100, step=50, label="â© ç»“æŸä½ç½®åç§» | End Offset (ms)")
                with gr.Tab("ğŸµ è‡ªåŠ¨é…ä¹ | Auto Music"):
                    with gr.Row():
                        music_root_input = gr.Textbox(label="Music Root Dir", value="/remote-home/haoningwu/xiaohuang/FunClip/music")
                    
                    # [æ–°å¢] è‡ªå®šä¹‰éŸ³ä¹ä¸Šä¼ ç»„ä»¶
                    custom_bgm_input = gr.Audio(label="[å¯é€‰] ä¸Šä¼ è‡ªå®šä¹‰èƒŒæ™¯éŸ³ä¹ (è¦†ç›–è‡ªåŠ¨æ¨è)", type="filepath")
                    
                    music_btn = gr.Button("ç”Ÿæˆé…ä¹è§†é¢‘ (éœ€å…ˆæå–è¯­ä¹‰)", variant="primary")
                    music_out_video = gr.Video(label="é…ä¹ç»“æœ")
                    music_log = gr.Textbox(label="æ—¥å¿—")

                    music_btn.click(
                        run_music, 
                        inputs=[video_input, music_root_input, semantic_state_data, custom_bgm_input], # å¢åŠ äº† custom_bgm_input
                        outputs=[music_out_video, music_log]
                    )
                
                # ğŸ¬ é¢„è§ˆä¸å¯¼å‡º Tab
                preview_export_components = create_integrated_preview_export_ui()
                    
                with gr.Row():
                    font_size = gr.Slider(minimum=10, maximum=100, value=32, step=2, label="ğŸ”  å­—å¹•å­—ä½“å¤§å° | Subtitle Font Size")
                    font_color = gr.Radio(["black", "white", "green", "red"], label="ğŸŒˆ å­—å¹•é¢œè‰² | Subtitle Color", value='white')
                    # font = gr.Radio(["é»‘ä½“", "Alibaba Sans"], label="å­—ä½“ Font")
                video_output = gr.Video(label="è£å‰ªç»“æœ | Video Clipped")
                audio_output = gr.Audio(label="è£å‰ªç»“æœ | Audio Clipped")
                clip_message = gr.Textbox(label="âš ï¸ è£å‰ªä¿¡æ¯ | Clipping Log")
                srt_clipped = gr.Textbox(label="ğŸ“– è£å‰ªéƒ¨åˆ†SRTå­—å¹•å†…å®¹ | Clipped RST Subtitles")            
                
        recog_button.click(mix_recog, 
                            inputs=[video_input, 
                                    audio_input, 
                                    hotwords_input, 
                                    output_dir,
                                    ], 
                            outputs=[video_text_output, video_srt_output, video_state, audio_state])
        recog_button2.click(mix_recog_speaker, 
                            inputs=[video_input, 
                                    audio_input, 
                                    hotwords_input, 
                                    output_dir,
                                    ], 
                            outputs=[video_text_output, video_srt_output, video_state, audio_state])
        clip_button.click(mix_clip, 
                           inputs=[video_text_input, 
                                   video_spk_input, 
                                   video_start_ost, 
                                   video_end_ost, 
                                   video_state, 
                                   audio_state, 
                                   output_dir
                                   ],
                           outputs=[video_output, audio_output, clip_message, srt_clipped])
        clip_subti_button.click(video_clip_addsub, 
                           inputs=[video_text_input, 
                                   video_spk_input, 
                                   video_start_ost, 
                                   video_end_ost, 
                                   video_state, 
                                   output_dir, 
                                   font_size, 
                                   font_color,
                                   ], 
                           outputs=[video_output, clip_message, srt_clipped])
        # semantic_button.click(video_semantic_understanding,
        #                     inputs=[video_input],
        #                     outputs=[video_semantic_output])
        semantic_button.click(
            video_semantic_understanding_wrapper,
            inputs=[video_input],
            outputs=[video_semantic_output, semantic_state_data] 
        )
        llm_button.click(llm_inference,
                         inputs=[prompt_head, prompt_head2, video_srt_output, llm_model, apikey_input],
                         outputs=[llm_result])
        llm_clip_button.click(AI_clip, 
                           inputs=[llm_result,
                                   video_text_input, 
                                   video_spk_input, 
                                   video_start_ost, 
                                   video_end_ost, 
                                   video_state, 
                                   audio_state, 
                                   output_dir,
                                   ],
                           outputs=[video_output, audio_output, clip_message, srt_clipped])
        llm_clip_subti_button.click(AI_clip_subti, 
                           inputs=[llm_result,
                                   video_text_input, 
                                   video_spk_input, 
                                   video_start_ost, 
                                   video_end_ost, 
                                   video_state, 
                                   audio_state, 
                                   output_dir,
                                   ],
                           outputs=[video_output, audio_output, clip_message, srt_clipped])
        
        # ğŸ¬ é¢„è§ˆä¸å¯¼å‡ºåŠŸèƒ½çš„å›è°ƒç»‘å®š
        ui_manager = preview_export_components['ui_manager']
        
        # åˆ·æ–°é¢„è§ˆ - å½“è§†é¢‘è¾“å‡ºæ›´æ–°æ—¶è‡ªåŠ¨åˆ·æ–°
        def auto_refresh_preview(video_path):
            return ui_manager.handle_preview_update(video_path)

        # ä½¿ç”¨å¯¼å‡ºç»“æœåˆ·æ–°é¢„è§ˆ
        def auto_refresh_preview_from_export(video_path):
            return ui_manager.handle_preview_update(video_path)
        
        # ç»‘å®šåˆ·æ–°é¢„è§ˆæŒ‰é’®
        preview_export_components['refresh_preview_btn'].click(
            auto_refresh_preview,
            inputs=[video_output],
            outputs=[
                preview_export_components['preview_video'],
                preview_export_components['video_info_text']
            ]
        )
        
        # å½“video_outputæ›´æ–°æ—¶ï¼Œè‡ªåŠ¨åˆ·æ–°é¢„è§ˆ
        video_output.change(
            auto_refresh_preview,
            inputs=[video_output],
            outputs=[
                preview_export_components['preview_video'],
                preview_export_components['video_info_text']
            ]
        )

        # å½“å¯¼å‡ºç»“æœæ›´æ–°æ—¶ï¼Œä¹ŸåŒæ­¥åˆ·æ–°é¢„è§ˆï¼ˆä¾¿äºæŸ¥çœ‹ä¸åŒå¯¼å‡ºå‚æ•°æ•ˆæœï¼‰
        preview_export_components['export_video_output'].change(
            auto_refresh_preview_from_export,
            inputs=[preview_export_components['export_video_output']],
            outputs=[
                preview_export_components['preview_video'],
                preview_export_components['video_info_text']
            ]
        )
        
        # ç»‘å®šå¯¼å‡ºæŒ‰é’®
        preview_export_components['export_btn'].click(
            ui_manager.handle_export,
            inputs=[
                video_output,  # ä½¿ç”¨å½“å‰è£å‰ªç»“æœ
                preview_export_components['resolution'],
                preview_export_components['platform'],
                preview_export_components['output_path'],
                preview_export_components['custom_width'],
                preview_export_components['custom_height'],
                preview_export_components['custom_bitrate'],
                preview_export_components['custom_fps']
            ],
            outputs=[
                preview_export_components['export_video_output'],
                preview_export_components['export_message'],
                preview_export_components['preview_video'],
                preview_export_components['video_info_text']
            ]
        )

        # ç»‘å®šå¯¼å‡ºé¢„è§ˆæŒ‰é’®ï¼ˆä»…ç”Ÿæˆå‰3ç§’è½»é‡é¢„è§ˆï¼‰
        preview_export_components['export_preview_btn'].click(
            ui_manager.handle_export_preview,
            inputs=[
                video_output,
                preview_export_components['resolution'],
                preview_export_components['platform'],
                preview_export_components['custom_width'],
                preview_export_components['custom_height'],
                preview_export_components['custom_bitrate'],
                preview_export_components['custom_fps']
            ],
            outputs=[
                preview_export_components['preview_video'],
                preview_export_components['video_info_text'],
                preview_export_components['export_message']
            ]
        )
        
        # ç»‘å®šæ‰¹é‡å¯¼å‡ºæŒ‰é’®
        preview_export_components['batch_export_btn'].click(
            ui_manager.handle_batch_export,
            inputs=[
                video_output,
                preview_export_components['batch_resolutions'],
                preview_export_components['batch_platforms'],
                preview_export_components['batch_output_dir']
            ],
            outputs=[
                preview_export_components['export_video_output'],
                preview_export_components['export_message'],
                preview_export_components['preview_video'],
                preview_export_components['video_info_text']
            ]
        )
    
    # start gradio service in local or share
    if args.listen:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name, inbrowser=False)
    else:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name)
